<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani | An Introduction to Statistical Learning: With Applications in R | Cash Prokop-Weaver</title><meta name=keywords content="hastodo,reference,hastodo,reference"><meta name=description content="Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani, (NO_ITEM_DATA:jamesIntroductionStatisticalLearningApplications2013)
Summary Thoughts Notes Skeleton Preface Contents 1 Introduction 2 Statistical Learning 2.1 What Is Statistical Learning?
2.1.1 Why Estimate f? 2.1.2 How Do We Estimate f? 2.1.3 The Trade-Off Between Prediction Accuracy and Model Interpretability 2.1.4 Supervised Versus Unsupervised Learning 2.1.5 Regression Versus Classification Problems 2.2 Assessing Model Accuracy
2.2.1 Measuring the Quality of Fit 2.2.2 The Bias-Variance Trade-Off 2.2.3 The Classification Setting 2."><meta name=author content><link rel=canonical href=http://notes.cashpw.com/posts/94bcb9cb-d5b8-49d7-a169-891808910a65/><link crossorigin=anonymous href=/assets/css/stylesheet.min.9007c896ec996ba5ebf9426a26d82d6ed8dae8c200cf0b759a8a574a116b055b.css integrity="sha256-kAfIluyZa6Xr+UJqJtgtbtja6MIAzwt1mopXShFrBVs=" rel="preload stylesheet" as=style><link rel=icon href=http://notes.cashpw.com/favicon.ico><link rel=apple-touch-icon href=http://notes.cashpw.com/apple-touch-icon.png><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-69408538-1","auto"),ga("send","pageview"))</script><meta name=twitter:title content="Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani | An Introduction to Statistical Learning: With Applications in R | Cash Prokop-Weaver"><meta name=twitter:description content="Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani, (NO_ITEM_DATA:jamesIntroductionStatisticalLearningApplications2013)
Summary Thoughts Notes Skeleton Preface Contents 1 Introduction 2 Statistical Learning 2.1 What Is Statistical Learning?
2.1.1 Why Estimate f? 2.1.2 How Do We Estimate f? 2.1.3 The Trade-Off Between Prediction Accuracy and Model Interpretability 2.1.4 Supervised Versus Unsupervised Learning 2.1.5 Regression Versus Classification Problems 2.2 Assessing Model Accuracy
2.2.1 Measuring the Quality of Fit 2.2.2 The Bias-Variance Trade-Off 2.2.3 The Classification Setting 2."><meta property="og:title" content="Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani | An Introduction to Statistical Learning: With Applications in R | Cash Prokop-Weaver"><meta property="og:description" content="Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani, (NO_ITEM_DATA:jamesIntroductionStatisticalLearningApplications2013)
Summary Thoughts Notes Skeleton Preface Contents 1 Introduction 2 Statistical Learning 2.1 What Is Statistical Learning?
2.1.1 Why Estimate f? 2.1.2 How Do We Estimate f? 2.1.3 The Trade-Off Between Prediction Accuracy and Model Interpretability 2.1.4 Supervised Versus Unsupervised Learning 2.1.5 Regression Versus Classification Problems 2.2 Assessing Model Accuracy
2.2.1 Measuring the Quality of Fit 2.2.2 The Bias-Variance Trade-Off 2.2.3 The Classification Setting 2."><meta property="og:type" content="article"><meta property="og:url" content="http://notes.cashpw.com/posts/94bcb9cb-d5b8-49d7-a169-891808910a65/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-12-24T09:19:00-08:00"><meta property="article:modified_time" content="2023-12-23T12:11:08-08:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://notes.cashpw.com/posts/"},{"@type":"ListItem","position":2,"name":"Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani | An Introduction to Statistical Learning: With Applications in R","item":"http://notes.cashpw.com/posts/94bcb9cb-d5b8-49d7-a169-891808910a65/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani | An Introduction to Statistical Learning: With Applications in R | Cash Prokop-Weaver","name":"Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani | An Introduction to Statistical Learning: With Applications in R","description":"Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani, (NO_ITEM_DATA:jamesIntroductionStatisticalLearningApplications2013)\nSummary Thoughts Notes Skeleton Preface Contents 1 Introduction 2 Statistical Learning 2.1 What Is Statistical Learning?\n2.1.1 Why Estimate f? 2.1.2 How Do We Estimate f? 2.1.3 The Trade-Off Between Prediction Accuracy and Model Interpretability 2.1.4 Supervised Versus Unsupervised Learning 2.1.5 Regression Versus Classification Problems 2.2 Assessing Model Accuracy\n2.2.1 Measuring the Quality of Fit 2.2.2 The Bias-Variance Trade-Off 2.2.3 The Classification Setting 2.","keywords":["hastodo","reference","hastodo","reference"],"wordCount":"1220","inLanguage":"en","datePublished":"2022-12-24T09:19:00-08:00","dateModified":"2023-12-23T12:11:08-08:00","author":[{"@type":"Person","name":"Cash Weaver"}],"mainEntityOfPage":{"@type":"WebPage","@id":"http://notes.cashpw.com/posts/94bcb9cb-d5b8-49d7-a169-891808910a65/"},"publisher":{"@type":"Organization","name":"Cash Prokop-Weaver","logo":{"@type":"ImageObject","url":"http://notes.cashpw.com/favicon.ico"}}}</script><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript></head><body class="dark type-posts kind-page layout-" id=top><script data-no-instant>function switchTheme(e){switch(e){case"light":document.body.classList.remove("dark");break;case"dark":document.body.classList.add("dark");break;default:window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")}}function isDarkTheme(){return document.body.className.includes("dark")}function getPrefTheme(){return localStorage.getItem("pref-theme")}function setPrefTheme(e){switchTheme(e),localStorage.setItem("pref-theme",e)}const toggleThemeCallbacks={};toggleThemeCallbacks.main=e=>{setPrefTheme(e?"light":"dark")},window.addEventListener("toggle-theme",function(){const e=isDarkTheme();for(const t in toggleThemeCallbacks)toggleThemeCallbacks[t](e)});function toggleThemeListener(){window.dispatchEvent(new CustomEvent("toggle-theme"))}</script><script>(function(){const t="dark",e=getPrefTheme(),n=e||t;switchTheme(n)})()</script><header class=header><nav class=nav><div class=logo><a href=http://notes.cashpw.com/ accesskey=h title="Cash Prokop-Weaver (Alt + H)">Cash Prokop-Weaver</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><button id=menu-trigger aria-haspopup=menu aria-label="Menu Button"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"><line x1="3" y1="12" x2="21" y2="12"/><line x1="3" y1="6" x2="21" y2="6"/><line x1="3" y1="18" x2="21" y2="18"/></svg></button><ul id=menu class="menu hidden"><li><a href=http://notes.cashpw.com/archives/ title=Archives>Archives</a></li><li><a href=http://notes.cashpw.com/search/ title="Search (Alt + /)" data-no-instant accesskey=/>Search</a></li></ul></nav></header><main class="main post"><article class=post-single data-pagefind-body><header class=post-header><h1 class=post-title>Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani | An Introduction to Statistical Learning: With Applications in R</h1><div class=post-meta><span class=meta-item><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar" style="user-select:text"><rect x="3" y="4" width="18" height="18" rx="2" ry="2" style="user-select:text"/><line x1="16" y1="2" x2="16" y2="6" style="user-select:text"/><line x1="8" y1="2" x2="8" y2="6" style="user-select:text"/><line x1="3" y1="10" x2="21" y2="10" style="user-select:text"/></svg><span>2022-12-24</span></span><span class=meta-item><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentcolor" class="bi bi-pencil" viewBox="0 0 16 16"><path d="M12.146.146a.5.5.0 01.708.0l3 3a.5.5.0 010 .708l-10 10a.5.5.0 01-.168.11l-5 2a.5.5.0 01-.65-.65l2-5a.5.5.0 01.11-.168zM11.207 2.5 13.5 4.793 14.793 3.5 12.5 1.207zm1.586 3L10.5 3.207 4 9.707V10h.5a.5.5.0 01.5.5v.5h.5a.5.5.0 01.5.5v.5h.293zm-9.761 5.175-.106.106-1.528 3.821 3.821-1.528.106-.106A.5.5.0 015 12.5V12h-.5a.5.5.0 01-.5-.5V11h-.5a.5.5.0 01-.468-.325"/></svg>
<span>2023-12-23</span></span><span class=meta-item><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" stroke="currentcolor" stroke-width="2" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<span>6 min</span></span></div><div class="post-meta post-references"></div></header><div class="toc side left"><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#summary aria-label=Summary>Summary</a></li><li><a href=#thoughts aria-label=Thoughts>Thoughts</a></li><li><a href=#notes aria-label=Notes>Notes</a><ul><li><a href=#skeleton aria-label=Skeleton>Skeleton</a><ul><li><a href=#preface aria-label=Preface>Preface</a></li><li><a href=#contents aria-label=Contents>Contents</a></li><li><a href=#1-introduction aria-label="1 Introduction">1 Introduction</a></li><li><a href=#2-statistical-learning aria-label="2 Statistical Learning">2 Statistical Learning</a></li><li><a href=#3-linear-regression aria-label="3 Linear Regression">3 Linear Regression</a></li><li><a href=#4-classification aria-label="4 Classification">4 Classification</a></li><li><a href=#5-resampling-methods aria-label="5 Resampling Methods">5 Resampling Methods</a></li><li><a href=#6-linear-model-selection-and-regularization aria-label="6 Linear Model Selection and Regularization">6 Linear Model Selection and Regularization</a></li><li><a href=#7-moving-beyond-linearity aria-label="7 Moving Beyond Linearity">7 Moving Beyond Linearity</a></li><li><a href=#8-tree-based-methods aria-label="8 Tree-Based Methods">8 Tree-Based Methods</a></li><li><a href=#9-support-vector-machines aria-label="9 Support Vector Machines">9 Support Vector Machines</a></li><li><a href=#10-deep-learning aria-label="10 Deep Learning">10 Deep Learning</a></li><li><a href=#11-survival-analysis-and-censored-data aria-label="11 Survival Analysis and Censored Data">11 Survival Analysis and Censored Data</a></li><li><a href=#12-unsupervised-learning aria-label="12 Unsupervised Learning">12 Unsupervised Learning</a></li><li><a href=#13-multiple-testing aria-label="13 Multiple Testing">13 Multiple Testing</a></li><li><a href=#index aria-label=Index>Index</a></li></ul></li></ul></li><li><a href=#bibliography aria-label=Bibliography>Bibliography</a></li><li><a href=#references aria-label=References>References</a></li><li><a href=#backlinks aria-label=Backlinks>Backlinks</a></li></ul></div></details></div><div class=post-content><p><a href=/posts/f5ed47e7-5d7a-4d4f-9ed2-6817ca706b05/>Gareth James</a>, <a href=/posts/23a21efb-912c-46ff-84f6-5b3d68f96060/>Daniela Witten</a>, <a href=/posts/b2981e3a-4e5b-41b2-a040-2fb58a7735a5/>Trevor Hastie</a>, <a href=/posts/29b3cfe2-55ed-45d5-92e5-e604808b72bb/>Robert Tibshirani</a>, (NO_ITEM_DATA:jamesIntroductionStatisticalLearningApplications2013)</p><h2 id=summary>Summary<a hidden class=anchor aria-hidden=true href=#summary>¶</a></h2><h2 id=thoughts>Thoughts<a hidden class=anchor aria-hidden=true href=#thoughts>¶</a></h2><h2 id=notes>Notes<a hidden class=anchor aria-hidden=true href=#notes>¶</a></h2><h3 id=skeleton>Skeleton<a hidden class=anchor aria-hidden=true href=#skeleton>¶</a></h3><h4 id=preface>Preface<a hidden class=anchor aria-hidden=true href=#preface>¶</a></h4><h4 id=contents>Contents<a hidden class=anchor aria-hidden=true href=#contents>¶</a></h4><h4 id=1-introduction>1 Introduction<a hidden class=anchor aria-hidden=true href=#1-introduction>¶</a></h4><h4 id=2-statistical-learning>2 Statistical Learning<a hidden class=anchor aria-hidden=true href=#2-statistical-learning>¶</a></h4><ul><li><p>2.1 What Is Statistical Learning?</p><ul><li>2.1.1 Why Estimate f?</li></ul><ul><li>2.1.2 How Do We Estimate f?</li></ul><ul><li>2.1.3 The Trade-Off Between Prediction Accuracy and Model Interpretability</li></ul><ul><li>2.1.4 Supervised Versus Unsupervised Learning</li></ul><ul><li>2.1.5 Regression Versus Classification Problems</li></ul></li></ul><ul><li><p>2.2 Assessing Model Accuracy</p><ul><li>2.2.1 Measuring the Quality of Fit</li></ul><ul><li>2.2.2 The Bias-Variance Trade-Off</li></ul><ul><li>2.2.3 The Classification Setting</li></ul></li></ul><ul><li><p>2.3 Lab: Introduction to R</p><ul><li>2.3.1 Basic Commands</li></ul><ul><li>2.3.2 Graphics</li></ul><ul><li>2.3.3 Indexing Data</li></ul><ul><li>2.3.4 Loading Data</li></ul><ul><li>2.3.5 Additional Graphical and Numerical Summaries</li></ul></li></ul><ul><li>2.4 Exercises</li></ul><h4 id=3-linear-regression>3 Linear Regression<a hidden class=anchor aria-hidden=true href=#3-linear-regression>¶</a></h4><ul><li><p>3.1 Simple Linear Regression</p><ul><li>3.1.1 Estimating the Coefficients</li></ul><ul><li>3.1.2 Assessing the Accuracy of the Coefficients Estimates</li></ul><ul><li>3.1.3 Assessing the Accuracy of the Model</li></ul></li></ul><ul><li><p>3.2 Multiple Linear Regression</p><ul><li>3.2.1 Estimating the Regression Coefficients</li></ul><ul><li>3.2.2 Some Important Questions</li></ul></li></ul><ul><li><p>3.3 Other Considerations in the Regression Model</p><ul><li>3.3.1 Qualitative Predictors</li></ul><ul><li>3.3.2 Extensions of the Linear Model</li></ul><ul><li>3.3.3 Potential Problems</li></ul></li></ul><ul><li>3.4 The Marketing Plan</li></ul><ul><li>3.5 Comparison of Linear Regression with K-Nearest Neighbors</li></ul><ul><li><p>3.6 Lab: Linear Regression</p><ul><li>3.6.1 Libraries</li></ul><ul><li>3.6.2 Simple Linear Regression</li></ul><ul><li>3.6.3 Multiple Linear Regression</li></ul><ul><li>3.6.4 Interaction Terms</li></ul><ul><li>3.6.5 Non-linear Transformations of the Predictors</li></ul><ul><li>3.6.6 Qualitative Predictors</li></ul><ul><li>3.6.7 Writing Functions</li></ul></li></ul><ul><li>3.7 Exercises</li></ul><h4 id=4-classification>4 Classification<a hidden class=anchor aria-hidden=true href=#4-classification>¶</a></h4><ul><li>4.1 An Overview of Classification</li></ul><ul><li>4.2 Why Not Linear Regression?</li></ul><ul><li><p>4.3 Logistic Regression</p><ul><li>4.3.1 The Logistic Model</li></ul><ul><li>4.3.2 Estimating the Regression Coefficients</li></ul><ul><li>4.3.3 Making Predictions</li></ul><ul><li>4.3.4 Multiple Logistic Regression</li></ul><ul><li>4.3.5 Multinomial Logistic Regression</li></ul></li></ul><ul><li><p>4.4 Generative Models for Classification</p><ul><li>4.4.1 Linear Discriminant Analysis for p = 1</li></ul><ul><li>4.4.2 Linear Discriminant Analysis for p >1</li></ul><ul><li>4.4.3 Quadratic Discriminant Analysis</li></ul><ul><li>4.4.4 Naive Bayes</li></ul></li></ul><ul><li><p>4.5 A Comparison of Classification Methods</p><ul><li>4.5.1 An Analytical Comparison</li></ul><ul><li>4.5.2 An Empirical Comparison</li></ul></li></ul><ul><li><p>4.6 Generalized Linear Models</p><ul><li>4.6.1 Linear Regression on the Bikeshare Data</li></ul><ul><li>4.6.2 Poisson Regression on the Bikeshare Data</li></ul><ul><li>4.6.3 Generalized Linear Models in Greater Generality</li></ul></li></ul><ul><li><p>4.7 Lab: Classification Methods</p><ul><li>4.7.1 The Stock Market Data</li></ul><ul><li>4.7.2 Logistic Regression</li></ul><ul><li>4.7.3 Linear Discriminant Analysis</li></ul><ul><li>4.7.4 Quadratic Discriminant Analysis</li></ul><ul><li>4.7.5 Naive Bayes</li></ul><ul><li>4.7.6 K-Nearest Neighbors</li></ul><ul><li>4.7.7 Poisson Regression</li></ul></li></ul><ul><li>4.8 Exercises</li></ul><h4 id=5-resampling-methods>5 Resampling Methods<a hidden class=anchor aria-hidden=true href=#5-resampling-methods>¶</a></h4><ul><li><p>5.1 Cross-Validation</p><ul><li>5.1.1 The Validation Set Approach</li></ul><ul><li>5.1.2 Leave-One-Out Cross-Validation</li></ul><ul><li>5.1.3 k-Fold Cross-Validation</li></ul><ul><li>5.1.4 Bias-Variance Trade-Off for k-Fold Cross-Validation</li></ul><ul><li>5.1.5 Cross-Validation on Classification Problems</li></ul></li></ul><ul><li>5.2 The Bootstrap</li></ul><ul><li><p>5.3 Lab: Cross-Validation and the Bootstrap</p><ul><li>5.3.1 The Validation Set Approach</li></ul><ul><li>5.3.2 Leave-One-Out Cross-Validation</li></ul><ul><li>5.3.3 k-Fold Cross-Validation</li></ul><ul><li>5.3.4 The Bootstrap</li></ul></li></ul><ul><li>5.4 Exercises</li></ul><h4 id=6-linear-model-selection-and-regularization>6 Linear Model Selection and Regularization<a hidden class=anchor aria-hidden=true href=#6-linear-model-selection-and-regularization>¶</a></h4><ul><li><p>6.1 Subset Selection</p><ul><li>6.1.1 Best Subset Selection</li></ul><ul><li>6.1.2 Stepwise Selection</li></ul><ul><li>6.1.3 Choosing the Optimal Model</li></ul></li></ul><ul><li><p>6.2 Shrinkage Methods</p><ul><li>6.2.1 Ridge Regression</li></ul><ul><li>6.2.2 The Lasso</li></ul><ul><li>6.2.3 Selecting the Tuning Parameter</li></ul></li></ul><ul><li><p>6.3 Dimension Reduction Methods</p><ul><li>6.3.1 Principal Components Regression</li></ul><ul><li>6.3.2 Partial Least Squares</li></ul></li></ul><ul><li><p>6.4 Considerations in High Dimensions</p><ul><li>6.4.1 High-Dimensional Data</li></ul><ul><li>6.4.2 What Goes Wrong in High Dimensions?</li></ul><ul><li>6.4.3 Regression in High Dimensions</li></ul><ul><li>6.4.4 Interpreting Results in High Dimensions</li></ul></li></ul><ul><li><p>6.5 Lab: Linear Models and Regularization Methods</p><ul><li>6.5.1 Subset Selection Methods</li></ul><ul><li>6.5.2 Ridge Regression and the Lasso</li></ul><ul><li>6.5.3 PCR and PLS Regression</li></ul></li></ul><ul><li>6.6 Exercises</li></ul><h4 id=7-moving-beyond-linearity>7 Moving Beyond Linearity<a hidden class=anchor aria-hidden=true href=#7-moving-beyond-linearity>¶</a></h4><ul><li>7.1 Polynomial Regression</li></ul><ul><li>7.2 Step Functions</li></ul><ul><li>7.3 Basis Functions</li></ul><ul><li><p>7.4 Regression Splines</p><ul><li>7.4.1 Piecewise Polynomials</li></ul><ul><li>7.4.2 Constraints and Splines</li></ul><ul><li>7.4.3 The Spline Basis Representation</li></ul><ul><li>7.4.4 Choosing the Number and Locations of the Knots</li></ul><ul><li>7.4.5 Comparison to Polynomial Regression</li></ul></li></ul><ul><li><p>7.5 Smoothing Splines</p><ul><li>7.5.1 An Overview of Smoothing Splines</li></ul><ul><li>7.5.2 Choosing the Smoothing Parameter λ</li></ul></li></ul><ul><li>7.6 Local Regression</li></ul><ul><li><p>7.7 Generalized Additive Models</p><ul><li>7.7.1 GAMs for Regression Problems</li></ul><ul><li>7.7.2 GAMs for Classification Problems</li></ul></li></ul><ul><li><p>7.8 Lab: Non-linear Modeling</p><ul><li>7.8.1 Polynomial Regression and Step Functions</li></ul><ul><li>7.8.2 Splines</li></ul><ul><li>7.8.3 GAMs</li></ul></li></ul><ul><li>7.9 Exercises</li></ul><h4 id=8-tree-based-methods>8 Tree-Based Methods<a hidden class=anchor aria-hidden=true href=#8-tree-based-methods>¶</a></h4><ul><li><p>8.1 The Basics of Decision Trees</p><ul><li>8.1.1 Regression Trees</li></ul><ul><li>8.1.2 Classification Trees</li></ul><ul><li>8.1.3 Trees Versus Linear Models</li></ul><ul><li>8.1.4 Advantages and Disadvantages of Trees</li></ul></li></ul><ul><li><p>8.2 Bagging, Random Forests, Boosting, and Bayesian Additive Regression Trees</p><ul><li>8.2.1 Bagging</li></ul><ul><li>8.2.2 Random Forests</li></ul><ul><li>8.2.3 Boosting</li></ul><ul><li>8.2.4 Bayesian Additive Regression Trees</li></ul><ul><li>8.2.5 Summary of Tree Ensemble Methods</li></ul></li></ul><ul><li><p>8.3 Lab: Decision Trees</p><ul><li>8.3.1 Fitting Classification Trees</li></ul><ul><li>8.3.2 Fitting Regression Trees</li></ul><ul><li>8.3.3 Bagging and Random Forests</li></ul><ul><li>8.3.4 Boosting</li></ul><ul><li>8.3.5 Bayesian Additive Regression Trees</li></ul></li></ul><ul><li>8.4 Exercises</li></ul><h4 id=9-support-vector-machines>9 Support Vector Machines<a hidden class=anchor aria-hidden=true href=#9-support-vector-machines>¶</a></h4><ul><li><p>9.1 Maximal Margin Classifier</p><ul><li>9.1.1 What Is a Hyperplane?</li></ul><ul><li>9.1.2 Classification Using a Separating Hyperplane</li></ul><ul><li>9.1.3 The Maximal Margin Classifier</li></ul><ul><li>9.1.4 Construction of the Maximal Margin Classifier</li></ul><ul><li>9.1.5 The Non-separable Case</li></ul></li></ul><ul><li><p>9.2 Support Vector Classifiers</p><ul><li>9.2.1 Overview of the Support Vector Classifier</li></ul><ul><li>9.2.2 Details of the Support Vector Classifier</li></ul></li></ul><ul><li><p>9.3 Support Vector Machines</p><ul><li>9.3.1 Classification with Non-Linear Decision Boundaries</li></ul><ul><li>9.3.2 The Support Vector Machine</li></ul><ul><li>9.3.3 An Application to the Heart Disease Data</li></ul></li></ul><ul><li><p>9.4 SVMs with More than Two Classes</p><ul><li>9.4.1 One-Versus-One Classification</li></ul><ul><li>9.4.2 One-Versus-All Classification</li></ul></li></ul><ul><li>9.5 Relationship to Logistic Regression</li></ul><ul><li><p>9.6 Lab: Support Vector Machines</p><ul><li>9.6.1 Support Vector Classifier</li></ul><ul><li>9.6.2 Support Vector Machine</li></ul><ul><li>9.6.3 ROC Curves</li></ul><ul><li>9.6.4 SVM with Multiple Classes</li></ul><ul><li>9.6.5 Application to Gene Expression Data</li></ul></li></ul><ul><li>9.7 Exercises</li></ul><h4 id=10-deep-learning>10 Deep Learning<a hidden class=anchor aria-hidden=true href=#10-deep-learning>¶</a></h4><ul><li>10.1 Single Layer Neural Networks</li></ul><ul><li>10.2 Multilayer Neural Networks</li></ul><ul><li><p>10.3 Convolutional Neural Networks</p><ul><li>10.3.1 Convolution Layers</li></ul><ul><li>10.3.2 Pooling Layers</li></ul><ul><li>10.3.3 Architecture of a Convolutional Neural Network</li></ul><ul><li>10.3.4 Data Augmentation</li></ul><ul><li>10.3.5 Results Using a Pretrained Classifier</li></ul></li></ul><ul><li>10.4 Document Classification</li></ul><ul><li><p>10.5 Recurrent Neural Networks</p><ul><li>10.5.1 Sequential Models for Document Classification</li></ul><ul><li>10.5.2 Time Series Forecasting</li></ul><ul><li>10.5.3 Summary of RNNs</li></ul></li></ul><ul><li>10.6 When to Use Deep Learning</li></ul><ul><li><p>10.7 Fitting a Neural Network</p><ul><li>10.7.1 Backpropagation</li></ul><ul><li>10.7.2 Regularization and Stochastic Gradient Descent</li></ul><ul><li>10.7.3 Dropout Learning</li></ul><ul><li>10.7.4 Network Tuning</li></ul></li></ul><ul><li>10.8 Interpolation and Double Descent</li></ul><ul><li><p>10.9 Lab: Deep Learning</p><ul><li>10.9.1 A Single Layer Network on the Hitters Data</li></ul><ul><li>10.9.2 A Multilayer Network on the MNIST Digit Data</li></ul><ul><li>10.9.3 Convolutional Neural Networks</li></ul><ul><li>10.9.4 Using Pretrained CNN Models</li></ul><ul><li>10.9.5 IMDb Document Classification</li></ul><ul><li>10.9.6 Recurrent Neural Networks</li></ul></li></ul><ul><li>10.10 Exercises</li></ul><h4 id=11-survival-analysis-and-censored-data>11 Survival Analysis and Censored Data<a hidden class=anchor aria-hidden=true href=#11-survival-analysis-and-censored-data>¶</a></h4><ul><li>11.1 Survival and Censoring Times</li></ul><ul><li>11.2 A Closer Look at Censoring</li></ul><ul><li>11.3 The Kaplan-Meier Survival Curve</li></ul><ul><li>11.4 The Log-Rank Test</li></ul><ul><li><p>11.5 Regression Models With a Survival Response</p><ul><li>11.5.1 The Hazard Function</li></ul><ul><li>11.5.2 Proportional Hazards</li></ul><ul><li>11.5.3 Example: Brain Cancer Data</li></ul><ul><li>11.5.4 Example: Publication Data</li></ul></li></ul><ul><li>11.6 Shrinkage for the Cox Model</li></ul><ul><li><p>11.7 Additional Topics</p><ul><li>11.7.1 Area Under the Curve for Survival Analysis</li></ul><ul><li>11.7.2 Choice of Time Scale</li></ul><ul><li>11.7.3 Time-Dependent Covariates</li></ul><ul><li>11.7.4 Checking the Proportional Hazards Assumption</li></ul><ul><li>11.7.5 Survival Trees</li></ul></li></ul><ul><li><p>11.8 Lab: Survival Analysis</p><ul><li>11.8.1 Brain Cancer Data</li></ul><ul><li>11.8.2 Publication Data</li></ul><ul><li>11.8.3 Call Center Data</li></ul></li></ul><ul><li>11.9 Exercises</li></ul><h4 id=12-unsupervised-learning>12 Unsupervised Learning<a hidden class=anchor aria-hidden=true href=#12-unsupervised-learning>¶</a></h4><ul><li>12.1 The Challenge of Unsupervised Learning</li></ul><ul><li><p>12.2 Principal Components Analysis</p><ul><li>12.2.1 What Are Principal Components?</li></ul><ul><li>12.2.2 Another Interpretation of Principal Components</li></ul><ul><li>12.2.3 The Proportion of Variance Explained</li></ul><ul><li>12.2.4 More on PCA</li></ul><ul><li>12.2.5 Other Uses for Principal Components</li></ul></li></ul><ul><li>12.3 Missing Values and Matrix Completion</li></ul><ul><li><p>12.4 Clustering Methods</p><ul><li>12.4.1 K-Means Clustering</li></ul><ul><li>12.4.2 Hierarchical Clustering</li></ul><ul><li>12.4.3 Practical Issues in Clustering</li></ul></li></ul><ul><li><p>12.5 Lab: Unsupervised Learning</p><ul><li>12.5.1 Principal Components Analysis</li></ul><ul><li>12.5.2 Matrix Completion</li></ul><ul><li>12.5.3 Clustering</li></ul><ul><li>12.5.4 NCI60 Data Example</li></ul></li></ul><ul><li>12.6 Exercises</li></ul><h4 id=13-multiple-testing>13 Multiple Testing<a hidden class=anchor aria-hidden=true href=#13-multiple-testing>¶</a></h4><ul><li><p>13.1 A Quick Review of Hypothesis Testing</p><ul><li>13.1.1 Testing a Hypothesis</li></ul><ul><li>13.1.2 Type I and Type II Errors</li></ul></li></ul><ul><li>13.2 The Challenge of Multiple Testing</li></ul><ul><li><p>13.3 The Family-Wise Error Rate</p><ul><li>13.3.1 What is the Family-Wise Error Rate?</li></ul><ul><li>13.3.2 Approaches to Control the Family-Wise Error Rate</li></ul><ul><li>13.3.3 Trade-Off Between the FWER and Power</li></ul></li></ul><ul><li><p>13.4 The False Discovery Rate</p><ul><li>13.4.1 Intuition for the False Discovery Rate</li></ul><ul><li>13.4.2 The Benjamini-Hochberg Procedure</li></ul></li></ul><ul><li><p>13.5 A Re-Sampling Approach to p-Values and False Discovery Rates</p><ul><li>13.5.1 A Re-Sampling Approach to the p-Value</li></ul><ul><li>13.5.2 A Re-Sampling Approach to the False Discovery Rate</li></ul><ul><li>13.5.3 When Are Re-Sampling Approaches Useful?</li></ul></li></ul><ul><li><p>13.6 Lab: Multiple Testing</p><ul><li>13.6.1 Review of Hypothesis Tests</li></ul><ul><li>13.6.2 The Family-Wise Error Rate</li></ul><ul><li>13.6.3 The False Discovery Rate</li></ul><ul><li>13.6.4 A Re-Sampling Approach</li></ul></li></ul><ul><li>13.7 Exercises</li></ul><h4 id=index>Index<a hidden class=anchor aria-hidden=true href=#index>¶</a></h4><h2 id=bibliography>Bibliography<a hidden class=anchor aria-hidden=true href=#bibliography>¶</a></h2><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>¶</a></h2><style>.csl-entry{text-indent:-1.5em;margin-left:1.5em}</style><div class=csl-bib-body><div class=csl-entry>NO_ITEM_DATA:jamesIntroductionStatisticalLearningApplications2013</div></div><h2 id=backlinks>Backlinks<a hidden class=anchor aria-hidden=true href=#backlinks>¶</a></h2><ul><li><a href=/posts/23a21efb-912c-46ff-84f6-5b3d68f96060/>Daniela Witten</a></li><li><a href=/posts/f5ed47e7-5d7a-4d4f-9ed2-6817ca706b05/>Gareth James</a></li><li><a href=/posts/29b3cfe2-55ed-45d5-92e5-e604808b72bb/>Robert Tibshirani</a></li><li><a href=/posts/b2981e3a-4e5b-41b2-a040-2fb58a7735a5/>Trevor Hastie</a></li></ul></div><footer class=post-footer><script>const backlinkHeader=document.getElementById("backlinks");if(backlinkHeader){const t=document.createElement("h4");t.innerHTML=backlinkHeader.innerHTML;const n=backlinkHeader.nextElementSibling;console.log(n);const e=document.createElement("aside");e.setAttribute("class","backlinks toc side right"),backlinkHeader.parentNode.insertBefore(e,backlinkHeader),e.appendChild(t),e.appendChild(n),backlinkHeader.remove(),t.id="backlinks"}</script><script src=/js/citations/custom.js></script></footer></article></main><footer class=footer><span>&copy; 2024 <a href=http://notes.cashpw.com/>Cash Prokop-Weaver</a></span><span style=display:inline-block;margin-left:1em>
<a href=https://creativecommons.org/licenses/by-sa/4.0/>CC BY-SA</a></span>
<span style=display:inline-block;margin-left:1em>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
    <a href=https://github.com/reorx/hugo-PaperModX/ rel=noopener target=_blank>PaperModX</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>const menuTrigger=document.querySelector("#menu-trigger"),menu=document.querySelector(".menu");menuTrigger.addEventListener("click",function(){menu.classList.toggle("hidden")}),document.body.addEventListener("click",function(e){menuTrigger.contains(e.target)||menu.classList.add("hidden")});const mobileWidthCutoffInPixels=768;window.screen.width>mobileWidthCutoffInPixels&&document.querySelectorAll(".toc").forEach(e=>{e.querySelectorAll(":scope > details").forEach(e=>{e.setAttribute("open","")})})</script><script>(function(){const t=""=="1";if(t)return;let e=document.getElementById("theme-toggle");e.removeEventListener("click",toggleThemeListener),e.addEventListener("click",toggleThemeListener)})()</script><script>(function(){let e=document.getElementById("menu");e&&(e.scrollLeft=localStorage.getItem("menu-scroll-position"),e.onscroll=function(){localStorage.setItem("menu-scroll-position",e.scrollLeft)});const t=""=="1",n=""=="1";if(window.matchMedia("(prefers-reduced-motion: reduce)").matches||t||n)return;document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})})()</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>if(window.scrollListeners)for(const e of scrollListeners)window.removeEventListener("scroll",e);window.scrollListeners=[]</script><script src=/js/medium-zoom.min.js data-no-instant></script>
<script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerText="copy";function s(){t.innerText="copied!",setTimeout(()=>{t.innerText="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script><script>(function(){const a="1"=="1";if(!a)return;if(!document.querySelector(".toc")){console.log("no toc found, ignore toc scroll");return}const r=window.scrollListeners,t=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id]"),n="active";let e=t[0];o(e).classList.add(n);const c=()=>{const s=[];for(const e of t)if(l(e)<5)s.push(e);else break;s.length>0?newActiveHeading=s[s.length-1]:newActiveHeading=t[0],e!=newActiveHeading&&(o(e).classList.remove(n),e=newActiveHeading,o(e).classList.add(n))};let s=null;const i=()=>{s!==null&&clearTimeout(s),s=setTimeout(c,50)};window.addEventListener("scroll",i,!1),r.push(i);function o(e){const t=encodeURI(e.getAttribute("id")).toLowerCase();return document.querySelector(`.toc ul li a[href="#${t}"]`)}function l(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect();return t.top}})()</script></body></html>