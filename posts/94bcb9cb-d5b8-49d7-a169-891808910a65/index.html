<!doctype html><html lang=en dir=auto>
<head><meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta name=robots content="index, follow">
<title>Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani | An Introduction to Statistical Learning: With Applications in R | Cash Prokop-Weaver</title>
<meta name=keywords content="has-todo,reference,has-todo,reference">
<meta name=description content="Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani, (James et al. 2013)
Summary Thoughts Notes Skeleton Preface Contents 1 Introduction 2 Statistical Learning    2.1 What Is Statistical Learning?
  2.1.1 Why Estimate f?    2.1.2 How Do We Estimate f?    2.1.3 The Trade-Off Between Prediction Accuracy and Model Interpretability    2.1.4 Supervised Versus Unsupervised Learning    2.1.5 Regression Versus Classification Problems       2.">
<meta name=author content>
<link rel=canonical href=http://notes.cashpw.com/posts/94bcb9cb-d5b8-49d7-a169-891808910a65/>
<link crossorigin=anonymous href=/assets/css/stylesheet.min.b29398f29a599d5e3004230f1be93c6d0e635ee0513a370a3f70268eb91bd59d.css integrity="sha256-spOY8ppZnV4wBCMPG+k8bQ5jXuBROjcKP3Amjrkb1Z0=" rel="preload stylesheet" as=style>
<link rel=icon href=http://notes.cashpw.com/favicon.ico>
<link rel=apple-touch-icon href=http://notes.cashpw.com/apple-touch-icon.png>
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(a,e,f,g,b,c,d){a.GoogleAnalyticsObject=b,a[b]=a[b]||function(){(a[b].q=a[b].q||[]).push(arguments)},a[b].l=1*new Date,c=e.createElement(f),d=e.getElementsByTagName(f)[0],c.async=1,c.src=g,d.parentNode.insertBefore(c,d)}(window,document,'script','https://www.google-analytics.com/analytics.js','ga'),ga('create','UA-69408538-1','auto'),ga('send','pageview'))</script>
<meta name=twitter:title content="Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani | An Introduction to Statistical Learning: With Applications in R | Cash Prokop-Weaver">
<meta name=twitter:description content="Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani, (James et al. 2013)
Summary Thoughts Notes Skeleton Preface Contents 1 Introduction 2 Statistical Learning    2.1 What Is Statistical Learning?
  2.1.1 Why Estimate f?    2.1.2 How Do We Estimate f?    2.1.3 The Trade-Off Between Prediction Accuracy and Model Interpretability    2.1.4 Supervised Versus Unsupervised Learning    2.1.5 Regression Versus Classification Problems       2.">
<meta property="og:title" content="Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani | An Introduction to Statistical Learning: With Applications in R | Cash Prokop-Weaver">
<meta property="og:description" content="Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani, (James et al. 2013)
Summary Thoughts Notes Skeleton Preface Contents 1 Introduction 2 Statistical Learning    2.1 What Is Statistical Learning?
  2.1.1 Why Estimate f?    2.1.2 How Do We Estimate f?    2.1.3 The Trade-Off Between Prediction Accuracy and Model Interpretability    2.1.4 Supervised Versus Unsupervised Learning    2.1.5 Regression Versus Classification Problems       2.">
<meta property="og:type" content="article">
<meta property="og:url" content="http://notes.cashpw.com/posts/94bcb9cb-d5b8-49d7-a169-891808910a65/">
<meta property="article:section" content="posts">
<meta property="article:published_time" content="2022-12-24T09:19:00-08:00">
<meta property="article:modified_time" content="2023-07-25T10:42:12-07:00">
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://notes.cashpw.com/posts/"},{"@type":"ListItem","position":2,"name":"Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani | An Introduction to Statistical Learning: With Applications in R","item":"http://notes.cashpw.com/posts/94bcb9cb-d5b8-49d7-a169-891808910a65/"}]}</script>
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani | An Introduction to Statistical Learning: With Applications in R | Cash Prokop-Weaver","name":"Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani | An Introduction to Statistical Learning: With Applications in R","description":"Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani, (James et al. 2013)\nSummary Thoughts Notes Skeleton Preface Contents 1 Introduction 2 Statistical Learning    2.1 What Is Statistical Learning?\n  2.1.1 Why Estimate f?    2.1.2 How Do We Estimate f?    2.1.3 The Trade-Off Between Prediction Accuracy and Model Interpretability    2.1.4 Supervised Versus Unsupervised Learning    2.1.5 Regression Versus Classification Problems       2.","keywords":["has-todo","reference","has-todo","reference"],"wordCount":"1307","inLanguage":"en","datePublished":"2022-12-24T09:19:00-08:00","dateModified":"2023-07-25T10:42:12-07:00","author":[{"@type":"Person","name":"Cash Weaver"}],"mainEntityOfPage":{"@type":"WebPage","@id":"http://notes.cashpw.com/posts/94bcb9cb-d5b8-49d7-a169-891808910a65/"},"publisher":{"@type":"Organization","name":"Cash Prokop-Weaver","logo":{"@type":"ImageObject","url":"http://notes.cashpw.com/favicon.ico"}}}</script><script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$'],['\\[','\\]']],processEscapes:!0,processEnvironments:!0,macros:{bigo:["{\\href{/posts/big_o_notation}{O}}(#1)",1],littleo:["{\\href{/posts/little_o_notation}{o}}(#1)",1],bigtheta:["{\\href{/posts/big_theta_notation}{\\Theta}}(#1)",1],bigomega:["{\\href{/posts/big_omega_notation}{\\Omega}}(#1)",1]}},options:{skipHtmlTags:['script','noscript','style','textarea','pre']}},window.addEventListener('load',a=>{document.querySelectorAll("mjx-container").forEach(function(a){a.parentElement.classList+='has-jax'})})</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script>
<noscript>
<style>#theme-toggle,.top-link{display:none}</style>
</noscript>
</head>
<body class="dark type-posts kind-page layout-" id=top><script data-no-instant>function switchTheme(a){switch(a){case'light':document.body.classList.remove('dark');break;case'dark':document.body.classList.add('dark');break;default:window.matchMedia('(prefers-color-scheme: dark)').matches&&document.body.classList.add('dark')}}function isDarkTheme(){return document.body.className.includes("dark")}function getPrefTheme(){return localStorage.getItem("pref-theme")}function setPrefTheme(a){switchTheme(a),localStorage.setItem("pref-theme",a)}const toggleThemeCallbacks={};toggleThemeCallbacks.main=a=>{a?setPrefTheme('light'):setPrefTheme('dark')},window.addEventListener('toggle-theme',function(){const a=isDarkTheme();for(const b in toggleThemeCallbacks)toggleThemeCallbacks[b](a)});function toggleThemeListener(){window.dispatchEvent(new CustomEvent('toggle-theme'))}</script>
<script>(function(){const b='dark',a=getPrefTheme(),c=a||b;switchTheme(c)})()</script>
<header class=header>
<nav class=nav>
<div class=logo>
<a href=http://notes.cashpw.com/ accesskey=h title="Cash Prokop-Weaver (Alt + H)">Cash Prokop-Weaver</a>
<span class=logo-switches>
<button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg>
</button>
</span>
</div>
<ul id=menu>
<li>
<a href=http://notes.cashpw.com/archives/ title=Archives>Archives
</a>
</li>
<li>
<a href=http://notes.cashpw.com/search/ title="Search (Alt + /)" data-no-instant accesskey=/>Search
</a>
</li>
<li>
<a href=http://notes.cashpw.com/tags/ title=Tags>Tags
</a>
</li>
</ul>
</nav>
</header>
<main class="main post">
<article class=post-single>
<header class=post-header>
<h1 class=post-title>
Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani | An Introduction to Statistical Learning: With Applications in R
</h1>
<div class=post-meta><span title="2022-12-24 09:19:00 -0800 -0800">2022-12-24</span>&nbsp;·&nbsp;<span style=margin-right:.5ch></span><span title="2023-07-25 10:42:12 -0700 -0700">2023-07-25</span>&nbsp;·&nbsp;7 min&nbsp;·&nbsp;<a href=https://github.com/cashpw/notes/edit/main/gareth_james_daniela_witten_trevor_hastie_robert_tibshirani_an_introduction_to_statistical_learning_with_applications_in_r.org rel="noopener noreferrer" target=_blank>Edit</a>
</div>
<div class="post-meta post-references"></div><div class="post-meta post-stars"></div>
</header> <div class="toc side left">
<details open>
<summary accesskey=c title="(Alt + C)">
<span class=details>Table of Contents</span>
</summary>
<div class=inner><ul>
<li>
<a href=#summary aria-label=Summary>Summary</a></li>
<li class=menu-link-thoughts>
<a href=#thoughts aria-label=Thoughts>Thoughts</a></li>
<li class=menu-link-notes>
<a href=#notes aria-label=Notes>Notes</a><ul>
<li class=menu-link-skeleton>
<a href=#skeleton aria-label=Skeleton>Skeleton</a><ul>
<li class=menu-link-preface>
<a href=#preface aria-label=Preface>Preface</a></li>
<li class=menu-link-contents>
<a href=#contents aria-label=Contents>Contents</a></li>
<li class=menu-link-1-introduction>
<a href=#1-introduction aria-label="1 Introduction">1 Introduction</a></li>
<li class=menu-link-2-statistical-learning>
<a href=#2-statistical-learning aria-label="2 Statistical Learning">2 Statistical Learning</a></li>
<li class=menu-link-3-linear-regression>
<a href=#3-linear-regression aria-label="3 Linear Regression">3 Linear Regression</a></li>
<li class=menu-link-4-classification>
<a href=#4-classification aria-label="4 Classification">4 Classification</a></li>
<li class=menu-link-5-resampling-methods>
<a href=#5-resampling-methods aria-label="5 Resampling Methods">5 Resampling Methods</a></li>
<li class=menu-link-6-linear-model-selection-and-regularization>
<a href=#6-linear-model-selection-and-regularization aria-label="6 Linear Model Selection and Regularization">6 Linear Model Selection and Regularization</a></li>
<li class=menu-link-7-moving-beyond-linearity>
<a href=#7-moving-beyond-linearity aria-label="7 Moving Beyond Linearity">7 Moving Beyond Linearity</a></li>
<li class=menu-link-8-tree-based-methods>
<a href=#8-tree-based-methods aria-label="8 Tree-Based Methods">8 Tree-Based Methods</a></li>
<li class=menu-link-9-support-vector-machines>
<a href=#9-support-vector-machines aria-label="9 Support Vector Machines">9 Support Vector Machines</a></li>
<li class=menu-link-10-deep-learning>
<a href=#10-deep-learning aria-label="10 Deep Learning">10 Deep Learning</a></li>
<li class=menu-link-11-survival-analysis-and-censored-data>
<a href=#11-survival-analysis-and-censored-data aria-label="11 Survival Analysis and Censored Data">11 Survival Analysis and Censored Data</a></li>
<li class=menu-link-12-unsupervised-learning>
<a href=#12-unsupervised-learning aria-label="12 Unsupervised Learning">12 Unsupervised Learning</a></li>
<li class=menu-link-13-multiple-testing>
<a href=#13-multiple-testing aria-label="13 Multiple Testing">13 Multiple Testing</a></li>
<li class=menu-link-index>
<a href=#index aria-label=Index>Index</a></li></ul>
</li></ul>
</li>
<li class=menu-link-flashcards>
<a href=#flashcards aria-label=Flashcards>Flashcards</a><ul>
<li class=menu-link-source>
<a href=#source aria-label=Source>Source</a><ul>
<li class=menu-link-back>
<a href=#back aria-label=Back>Back</a></li></ul>
</li>
<li class=menu-link-aka>
<a href=#aka aria-label=AKA>AKA</a></li></ul>
</li>
<li class=menu-link-references>
<a href=#references aria-label=References>References</a></li>
<li class=menu-link-backlinks>
<a href=#backlinks aria-label=Backlinks>Backlinks</a>
</li>
</ul>
</div>
</details>
</div>
<div class=post-content>
<p><a href=/posts/f5ed47e7-5d7a-4d4f-9ed2-6817ca706b05/>Gareth James</a>, <a href=/posts/23a21efb-912c-46ff-84f6-5b3d68f96060/>Daniela Witten</a>, <a href=/posts/b2981e3a-4e5b-41b2-a040-2fb58a7735a5/>Trevor Hastie</a>, <a href=/posts/29b3cfe2-55ed-45d5-92e5-e604808b72bb/>Robert Tibshirani</a>, (<a href=#citeproc_bib_item_1>James et al. 2013</a>)</p>
<h2 id=summary>Summary<a hidden class=anchor aria-hidden=true href=#summary>¶</a></h2>
<h2 id=thoughts>Thoughts<a hidden class=anchor aria-hidden=true href=#thoughts>¶</a></h2>
<h2 id=notes>Notes<a hidden class=anchor aria-hidden=true href=#notes>¶</a></h2>
<h3 id=skeleton>Skeleton<a hidden class=anchor aria-hidden=true href=#skeleton>¶</a></h3>
<h4 id=preface>Preface<a hidden class=anchor aria-hidden=true href=#preface>¶</a></h4>
<h4 id=contents>Contents<a hidden class=anchor aria-hidden=true href=#contents>¶</a></h4>
<h4 id=1-introduction>1 Introduction<a hidden class=anchor aria-hidden=true href=#1-introduction>¶</a></h4>
<h4 id=2-statistical-learning>2 Statistical Learning<a hidden class=anchor aria-hidden=true href=#2-statistical-learning>¶</a></h4>
<ul>
<li>
<p>2.1 What Is Statistical Learning?</p>
<ul>
<li>2.1.1 Why Estimate f?</li>
</ul>
<ul>
<li>2.1.2 How Do We Estimate f?</li>
</ul>
<ul>
<li>2.1.3 The Trade-Off Between Prediction Accuracy and Model Interpretability</li>
</ul>
<ul>
<li>2.1.4 Supervised Versus Unsupervised Learning</li>
</ul>
<ul>
<li>2.1.5 Regression Versus Classification Problems</li>
</ul>
</li>
</ul>
<ul>
<li>
<p>2.2 Assessing Model Accuracy</p>
<ul>
<li>2.2.1 Measuring the Quality of Fit</li>
</ul>
<ul>
<li>2.2.2 The Bias-Variance Trade-Off</li>
</ul>
<ul>
<li>2.2.3 The Classification Setting</li>
</ul>
</li>
</ul>
<ul>
<li>
<p>2.3 Lab: Introduction to R</p>
<ul>
<li>2.3.1 Basic Commands</li>
</ul>
<ul>
<li>2.3.2 Graphics</li>
</ul>
<ul>
<li>2.3.3 Indexing Data</li>
</ul>
<ul>
<li>2.3.4 Loading Data</li>
</ul>
<ul>
<li>2.3.5 Additional Graphical and Numerical Summaries</li>
</ul>
</li>
</ul>
<ul>
<li>2.4 Exercises</li>
</ul>
<h4 id=3-linear-regression>3 Linear Regression<a hidden class=anchor aria-hidden=true href=#3-linear-regression>¶</a></h4>
<ul>
<li>
<p>3.1 Simple Linear Regression</p>
<ul>
<li>3.1.1 Estimating the Coefficients</li>
</ul>
<ul>
<li>3.1.2 Assessing the Accuracy of the Coefficients Estimates</li>
</ul>
<ul>
<li>3.1.3 Assessing the Accuracy of the Model</li>
</ul>
</li>
</ul>
<ul>
<li>
<p>3.2 Multiple Linear Regression</p>
<ul>
<li>3.2.1 Estimating the Regression Coefficients</li>
</ul>
<ul>
<li>3.2.2 Some Important Questions</li>
</ul>
</li>
</ul>
<ul>
<li>
<p>3.3 Other Considerations in the Regression Model</p>
<ul>
<li>3.3.1 Qualitative Predictors</li>
</ul>
<ul>
<li>3.3.2 Extensions of the Linear Model</li>
</ul>
<ul>
<li>3.3.3 Potential Problems</li>
</ul>
</li>
</ul>
<ul>
<li>3.4 The Marketing Plan</li>
</ul>
<ul>
<li>3.5 Comparison of Linear Regression with K-Nearest Neighbors</li>
</ul>
<ul>
<li>
<p>3.6 Lab: Linear Regression</p>
<ul>
<li>3.6.1 Libraries</li>
</ul>
<ul>
<li>3.6.2 Simple Linear Regression</li>
</ul>
<ul>
<li>3.6.3 Multiple Linear Regression</li>
</ul>
<ul>
<li>3.6.4 Interaction Terms</li>
</ul>
<ul>
<li>3.6.5 Non-linear Transformations of the Predictors</li>
</ul>
<ul>
<li>3.6.6 Qualitative Predictors</li>
</ul>
<ul>
<li>3.6.7 Writing Functions</li>
</ul>
</li>
</ul>
<ul>
<li>3.7 Exercises</li>
</ul>
<h4 id=4-classification>4 Classification<a hidden class=anchor aria-hidden=true href=#4-classification>¶</a></h4>
<ul>
<li>4.1 An Overview of Classification</li>
</ul>
<ul>
<li>4.2 Why Not Linear Regression?</li>
</ul>
<ul>
<li>
<p>4.3 Logistic Regression</p>
<ul>
<li>4.3.1 The Logistic Model</li>
</ul>
<ul>
<li>4.3.2 Estimating the Regression Coefficients</li>
</ul>
<ul>
<li>4.3.3 Making Predictions</li>
</ul>
<ul>
<li>4.3.4 Multiple Logistic Regression</li>
</ul>
<ul>
<li>4.3.5 Multinomial Logistic Regression</li>
</ul>
</li>
</ul>
<ul>
<li>
<p>4.4 Generative Models for Classification</p>
<ul>
<li>4.4.1 Linear Discriminant Analysis for p = 1</li>
</ul>
<ul>
<li>4.4.2 Linear Discriminant Analysis for p >1</li>
</ul>
<ul>
<li>4.4.3 Quadratic Discriminant Analysis</li>
</ul>
<ul>
<li>4.4.4 Naive Bayes</li>
</ul>
</li>
</ul>
<ul>
<li>
<p>4.5 A Comparison of Classification Methods</p>
<ul>
<li>4.5.1 An Analytical Comparison</li>
</ul>
<ul>
<li>4.5.2 An Empirical Comparison</li>
</ul>
</li>
</ul>
<ul>
<li>
<p>4.6 Generalized Linear Models</p>
<ul>
<li>4.6.1 Linear Regression on the Bikeshare Data</li>
</ul>
<ul>
<li>4.6.2 Poisson Regression on the Bikeshare Data</li>
</ul>
<ul>
<li>4.6.3 Generalized Linear Models in Greater Generality</li>
</ul>
</li>
</ul>
<ul>
<li>
<p>4.7 Lab: Classification Methods</p>
<ul>
<li>4.7.1 The Stock Market Data</li>
</ul>
<ul>
<li>4.7.2 Logistic Regression</li>
</ul>
<ul>
<li>4.7.3 Linear Discriminant Analysis</li>
</ul>
<ul>
<li>4.7.4 Quadratic Discriminant Analysis</li>
</ul>
<ul>
<li>4.7.5 Naive Bayes</li>
</ul>
<ul>
<li>4.7.6 K-Nearest Neighbors</li>
</ul>
<ul>
<li>4.7.7 Poisson Regression</li>
</ul>
</li>
</ul>
<ul>
<li>4.8 Exercises</li>
</ul>
<h4 id=5-resampling-methods>5 Resampling Methods<a hidden class=anchor aria-hidden=true href=#5-resampling-methods>¶</a></h4>
<ul>
<li>
<p>5.1 Cross-Validation</p>
<ul>
<li>5.1.1 The Validation Set Approach</li>
</ul>
<ul>
<li>5.1.2 Leave-One-Out Cross-Validation</li>
</ul>
<ul>
<li>5.1.3 k-Fold Cross-Validation</li>
</ul>
<ul>
<li>5.1.4 Bias-Variance Trade-Off for k-Fold Cross-Validation</li>
</ul>
<ul>
<li>5.1.5 Cross-Validation on Classification Problems</li>
</ul>
</li>
</ul>
<ul>
<li>5.2 The Bootstrap</li>
</ul>
<ul>
<li>
<p>5.3 Lab: Cross-Validation and the Bootstrap</p>
<ul>
<li>5.3.1 The Validation Set Approach</li>
</ul>
<ul>
<li>5.3.2 Leave-One-Out Cross-Validation</li>
</ul>
<ul>
<li>5.3.3 k-Fold Cross-Validation</li>
</ul>
<ul>
<li>5.3.4 The Bootstrap</li>
</ul>
</li>
</ul>
<ul>
<li>5.4 Exercises</li>
</ul>
<h4 id=6-linear-model-selection-and-regularization>6 Linear Model Selection and Regularization<a hidden class=anchor aria-hidden=true href=#6-linear-model-selection-and-regularization>¶</a></h4>
<ul>
<li>
<p>6.1 Subset Selection</p>
<ul>
<li>6.1.1 Best Subset Selection</li>
</ul>
<ul>
<li>6.1.2 Stepwise Selection</li>
</ul>
<ul>
<li>6.1.3 Choosing the Optimal Model</li>
</ul>
</li>
</ul>
<ul>
<li>
<p>6.2 Shrinkage Methods</p>
<ul>
<li>6.2.1 Ridge Regression</li>
</ul>
<ul>
<li>6.2.2 The Lasso</li>
</ul>
<ul>
<li>6.2.3 Selecting the Tuning Parameter</li>
</ul>
</li>
</ul>
<ul>
<li>
<p>6.3 Dimension Reduction Methods</p>
<ul>
<li>6.3.1 Principal Components Regression</li>
</ul>
<ul>
<li>6.3.2 Partial Least Squares</li>
</ul>
</li>
</ul>
<ul>
<li>
<p>6.4 Considerations in High Dimensions</p>
<ul>
<li>6.4.1 High-Dimensional Data</li>
</ul>
<ul>
<li>6.4.2 What Goes Wrong in High Dimensions?</li>
</ul>
<ul>
<li>6.4.3 Regression in High Dimensions</li>
</ul>
<ul>
<li>6.4.4 Interpreting Results in High Dimensions</li>
</ul>
</li>
</ul>
<ul>
<li>
<p>6.5 Lab: Linear Models and Regularization Methods</p>
<ul>
<li>6.5.1 Subset Selection Methods</li>
</ul>
<ul>
<li>6.5.2 Ridge Regression and the Lasso</li>
</ul>
<ul>
<li>6.5.3 PCR and PLS Regression</li>
</ul>
</li>
</ul>
<ul>
<li>6.6 Exercises</li>
</ul>
<h4 id=7-moving-beyond-linearity>7 Moving Beyond Linearity<a hidden class=anchor aria-hidden=true href=#7-moving-beyond-linearity>¶</a></h4>
<ul>
<li>7.1 Polynomial Regression</li>
</ul>
<ul>
<li>7.2 Step Functions</li>
</ul>
<ul>
<li>7.3 Basis Functions</li>
</ul>
<ul>
<li>
<p>7.4 Regression Splines</p>
<ul>
<li>7.4.1 Piecewise Polynomials</li>
</ul>
<ul>
<li>7.4.2 Constraints and Splines</li>
</ul>
<ul>
<li>7.4.3 The Spline Basis Representation</li>
</ul>
<ul>
<li>7.4.4 Choosing the Number and Locations of the Knots</li>
</ul>
<ul>
<li>7.4.5 Comparison to Polynomial Regression</li>
</ul>
</li>
</ul>
<ul>
<li>
<p>7.5 Smoothing Splines</p>
<ul>
<li>7.5.1 An Overview of Smoothing Splines</li>
</ul>
<ul>
<li>7.5.2 Choosing the Smoothing Parameter λ</li>
</ul>
</li>
</ul>
<ul>
<li>7.6 Local Regression</li>
</ul>
<ul>
<li>
<p>7.7 Generalized Additive Models</p>
<ul>
<li>7.7.1 GAMs for Regression Problems</li>
</ul>
<ul>
<li>7.7.2 GAMs for Classification Problems</li>
</ul>
</li>
</ul>
<ul>
<li>
<p>7.8 Lab: Non-linear Modeling</p>
<ul>
<li>7.8.1 Polynomial Regression and Step Functions</li>
</ul>
<ul>
<li>7.8.2 Splines</li>
</ul>
<ul>
<li>7.8.3 GAMs</li>
</ul>
</li>
</ul>
<ul>
<li>7.9 Exercises</li>
</ul>
<h4 id=8-tree-based-methods>8 Tree-Based Methods<a hidden class=anchor aria-hidden=true href=#8-tree-based-methods>¶</a></h4>
<ul>
<li>
<p>8.1 The Basics of Decision Trees</p>
<ul>
<li>8.1.1 Regression Trees</li>
</ul>
<ul>
<li>8.1.2 Classification Trees</li>
</ul>
<ul>
<li>8.1.3 Trees Versus Linear Models</li>
</ul>
<ul>
<li>8.1.4 Advantages and Disadvantages of Trees</li>
</ul>
</li>
</ul>
<ul>
<li>
<p>8.2 Bagging, Random Forests, Boosting, and Bayesian Additive Regression Trees</p>
<ul>
<li>8.2.1 Bagging</li>
</ul>
<ul>
<li>8.2.2 Random Forests</li>
</ul>
<ul>
<li>8.2.3 Boosting</li>
</ul>
<ul>
<li>8.2.4 Bayesian Additive Regression Trees</li>
</ul>
<ul>
<li>8.2.5 Summary of Tree Ensemble Methods</li>
</ul>
</li>
</ul>
<ul>
<li>
<p>8.3 Lab: Decision Trees</p>
<ul>
<li>8.3.1 Fitting Classification Trees</li>
</ul>
<ul>
<li>8.3.2 Fitting Regression Trees</li>
</ul>
<ul>
<li>8.3.3 Bagging and Random Forests</li>
</ul>
<ul>
<li>8.3.4 Boosting</li>
</ul>
<ul>
<li>8.3.5 Bayesian Additive Regression Trees</li>
</ul>
</li>
</ul>
<ul>
<li>8.4 Exercises</li>
</ul>
<h4 id=9-support-vector-machines>9 Support Vector Machines<a hidden class=anchor aria-hidden=true href=#9-support-vector-machines>¶</a></h4>
<ul>
<li>
<p>9.1 Maximal Margin Classifier</p>
<ul>
<li>9.1.1 What Is a Hyperplane?</li>
</ul>
<ul>
<li>9.1.2 Classification Using a Separating Hyperplane</li>
</ul>
<ul>
<li>9.1.3 The Maximal Margin Classifier</li>
</ul>
<ul>
<li>9.1.4 Construction of the Maximal Margin Classifier</li>
</ul>
<ul>
<li>9.1.5 The Non-separable Case</li>
</ul>
</li>
</ul>
<ul>
<li>
<p>9.2 Support Vector Classifiers</p>
<ul>
<li>9.2.1 Overview of the Support Vector Classifier</li>
</ul>
<ul>
<li>9.2.2 Details of the Support Vector Classifier</li>
</ul>
</li>
</ul>
<ul>
<li>
<p>9.3 Support Vector Machines</p>
<ul>
<li>9.3.1 Classification with Non-Linear Decision Boundaries</li>
</ul>
<ul>
<li>9.3.2 The Support Vector Machine</li>
</ul>
<ul>
<li>9.3.3 An Application to the Heart Disease Data</li>
</ul>
</li>
</ul>
<ul>
<li>
<p>9.4 SVMs with More than Two Classes</p>
<ul>
<li>9.4.1 One-Versus-One Classification</li>
</ul>
<ul>
<li>9.4.2 One-Versus-All Classification</li>
</ul>
</li>
</ul>
<ul>
<li>9.5 Relationship to Logistic Regression</li>
</ul>
<ul>
<li>
<p>9.6 Lab: Support Vector Machines</p>
<ul>
<li>9.6.1 Support Vector Classifier</li>
</ul>
<ul>
<li>9.6.2 Support Vector Machine</li>
</ul>
<ul>
<li>9.6.3 ROC Curves</li>
</ul>
<ul>
<li>9.6.4 SVM with Multiple Classes</li>
</ul>
<ul>
<li>9.6.5 Application to Gene Expression Data</li>
</ul>
</li>
</ul>
<ul>
<li>9.7 Exercises</li>
</ul>
<h4 id=10-deep-learning>10 Deep Learning<a hidden class=anchor aria-hidden=true href=#10-deep-learning>¶</a></h4>
<ul>
<li>10.1 Single Layer Neural Networks</li>
</ul>
<ul>
<li>10.2 Multilayer Neural Networks</li>
</ul>
<ul>
<li>
<p>10.3 Convolutional Neural Networks</p>
<ul>
<li>10.3.1 Convolution Layers</li>
</ul>
<ul>
<li>10.3.2 Pooling Layers</li>
</ul>
<ul>
<li>10.3.3 Architecture of a Convolutional Neural Network</li>
</ul>
<ul>
<li>10.3.4 Data Augmentation</li>
</ul>
<ul>
<li>10.3.5 Results Using a Pretrained Classifier</li>
</ul>
</li>
</ul>
<ul>
<li>10.4 Document Classification</li>
</ul>
<ul>
<li>
<p>10.5 Recurrent Neural Networks</p>
<ul>
<li>10.5.1 Sequential Models for Document Classification</li>
</ul>
<ul>
<li>10.5.2 Time Series Forecasting</li>
</ul>
<ul>
<li>10.5.3 Summary of RNNs</li>
</ul>
</li>
</ul>
<ul>
<li>10.6 When to Use Deep Learning</li>
</ul>
<ul>
<li>
<p>10.7 Fitting a Neural Network</p>
<ul>
<li>10.7.1 Backpropagation</li>
</ul>
<ul>
<li>10.7.2 Regularization and Stochastic Gradient Descent</li>
</ul>
<ul>
<li>10.7.3 Dropout Learning</li>
</ul>
<ul>
<li>10.7.4 Network Tuning</li>
</ul>
</li>
</ul>
<ul>
<li>10.8 Interpolation and Double Descent</li>
</ul>
<ul>
<li>
<p>10.9 Lab: Deep Learning</p>
<ul>
<li>10.9.1 A Single Layer Network on the Hitters Data</li>
</ul>
<ul>
<li>10.9.2 A Multilayer Network on the MNIST Digit Data</li>
</ul>
<ul>
<li>10.9.3 Convolutional Neural Networks</li>
</ul>
<ul>
<li>10.9.4 Using Pretrained CNN Models</li>
</ul>
<ul>
<li>10.9.5 IMDb Document Classification</li>
</ul>
<ul>
<li>10.9.6 Recurrent Neural Networks</li>
</ul>
</li>
</ul>
<ul>
<li>10.10 Exercises</li>
</ul>
<h4 id=11-survival-analysis-and-censored-data>11 Survival Analysis and Censored Data<a hidden class=anchor aria-hidden=true href=#11-survival-analysis-and-censored-data>¶</a></h4>
<ul>
<li>11.1 Survival and Censoring Times</li>
</ul>
<ul>
<li>11.2 A Closer Look at Censoring</li>
</ul>
<ul>
<li>11.3 The Kaplan-Meier Survival Curve</li>
</ul>
<ul>
<li>11.4 The Log-Rank Test</li>
</ul>
<ul>
<li>
<p>11.5 Regression Models With a Survival Response</p>
<ul>
<li>11.5.1 The Hazard Function</li>
</ul>
<ul>
<li>11.5.2 Proportional Hazards</li>
</ul>
<ul>
<li>11.5.3 Example: Brain Cancer Data</li>
</ul>
<ul>
<li>11.5.4 Example: Publication Data</li>
</ul>
</li>
</ul>
<ul>
<li>11.6 Shrinkage for the Cox Model</li>
</ul>
<ul>
<li>
<p>11.7 Additional Topics</p>
<ul>
<li>11.7.1 Area Under the Curve for Survival Analysis</li>
</ul>
<ul>
<li>11.7.2 Choice of Time Scale</li>
</ul>
<ul>
<li>11.7.3 Time-Dependent Covariates</li>
</ul>
<ul>
<li>11.7.4 Checking the Proportional Hazards Assumption</li>
</ul>
<ul>
<li>11.7.5 Survival Trees</li>
</ul>
</li>
</ul>
<ul>
<li>
<p>11.8 Lab: Survival Analysis</p>
<ul>
<li>11.8.1 Brain Cancer Data</li>
</ul>
<ul>
<li>11.8.2 Publication Data</li>
</ul>
<ul>
<li>11.8.3 Call Center Data</li>
</ul>
</li>
</ul>
<ul>
<li>11.9 Exercises</li>
</ul>
<h4 id=12-unsupervised-learning>12 Unsupervised Learning<a hidden class=anchor aria-hidden=true href=#12-unsupervised-learning>¶</a></h4>
<ul>
<li>12.1 The Challenge of Unsupervised Learning</li>
</ul>
<ul>
<li>
<p>12.2 Principal Components Analysis</p>
<ul>
<li>12.2.1 What Are Principal Components?</li>
</ul>
<ul>
<li>12.2.2 Another Interpretation of Principal Components</li>
</ul>
<ul>
<li>12.2.3 The Proportion of Variance Explained</li>
</ul>
<ul>
<li>12.2.4 More on PCA</li>
</ul>
<ul>
<li>12.2.5 Other Uses for Principal Components</li>
</ul>
</li>
</ul>
<ul>
<li>12.3 Missing Values and Matrix Completion</li>
</ul>
<ul>
<li>
<p>12.4 Clustering Methods</p>
<ul>
<li>12.4.1 K-Means Clustering</li>
</ul>
<ul>
<li>12.4.2 Hierarchical Clustering</li>
</ul>
<ul>
<li>12.4.3 Practical Issues in Clustering</li>
</ul>
</li>
</ul>
<ul>
<li>
<p>12.5 Lab: Unsupervised Learning</p>
<ul>
<li>12.5.1 Principal Components Analysis</li>
</ul>
<ul>
<li>12.5.2 Matrix Completion</li>
</ul>
<ul>
<li>12.5.3 Clustering</li>
</ul>
<ul>
<li>12.5.4 NCI60 Data Example</li>
</ul>
</li>
</ul>
<ul>
<li>12.6 Exercises</li>
</ul>
<h4 id=13-multiple-testing>13 Multiple Testing<a hidden class=anchor aria-hidden=true href=#13-multiple-testing>¶</a></h4>
<ul>
<li>
<p>13.1 A Quick Review of Hypothesis Testing</p>
<ul>
<li>13.1.1 Testing a Hypothesis</li>
</ul>
<ul>
<li>13.1.2 Type I and Type II Errors</li>
</ul>
</li>
</ul>
<ul>
<li>13.2 The Challenge of Multiple Testing</li>
</ul>
<ul>
<li>
<p>13.3 The Family-Wise Error Rate</p>
<ul>
<li>13.3.1 What is the Family-Wise Error Rate?</li>
</ul>
<ul>
<li>13.3.2 Approaches to Control the Family-Wise Error Rate</li>
</ul>
<ul>
<li>13.3.3 Trade-Off Between the FWER and Power</li>
</ul>
</li>
</ul>
<ul>
<li>
<p>13.4 The False Discovery Rate</p>
<ul>
<li>13.4.1 Intuition for the False Discovery Rate</li>
</ul>
<ul>
<li>13.4.2 The Benjamini-Hochberg Procedure</li>
</ul>
</li>
</ul>
<ul>
<li>
<p>13.5 A Re-Sampling Approach to p-Values and False Discovery Rates</p>
<ul>
<li>13.5.1 A Re-Sampling Approach to the p-Value</li>
</ul>
<ul>
<li>13.5.2 A Re-Sampling Approach to the False Discovery Rate</li>
</ul>
<ul>
<li>13.5.3 When Are Re-Sampling Approaches Useful?</li>
</ul>
</li>
</ul>
<ul>
<li>
<p>13.6 Lab: Multiple Testing</p>
<ul>
<li>13.6.1 Review of Hypothesis Tests</li>
</ul>
<ul>
<li>13.6.2 The Family-Wise Error Rate</li>
</ul>
<ul>
<li>13.6.3 The False Discovery Rate</li>
</ul>
<ul>
<li>13.6.4 A Re-Sampling Approach</li>
</ul>
</li>
</ul>
<ul>
<li>13.7 Exercises</li>
</ul>
<h4 id=index>Index<a hidden class=anchor aria-hidden=true href=#index>¶</a></h4>
<h2 id=flashcards>Flashcards<a hidden class=anchor aria-hidden=true href=#flashcards>¶</a></h2>
<h3 id=source>Source<a hidden class=anchor aria-hidden=true href=#source>¶</a></h3>
<table>
<thead>
<tr>
<th>position</th>
<th>ease</th>
<th>box</th>
<th>interval</th>
<th>due</th>
</tr>
</thead>
<tbody>
<tr>
<td>front</td>
<td>2.65</td>
<td>13</td>
<td>377.11</td>
<td>2024-03-30T20:39:07Z</td>
</tr>
</tbody>
</table>
<p><a href=/posts/94bcb9cb-d5b8-49d7-a169-891808910a65/>An Introduction to Statistical Learning: With Applications in R</a></p>
<h4 id=back>Back<a hidden class=anchor aria-hidden=true href=#back>¶</a></h4>
<ol>
<li><a href=/posts/f5ed47e7-5d7a-4d4f-9ed2-6817ca706b05/>Gareth James</a></li>
<li><a href=/posts/23a21efb-912c-46ff-84f6-5b3d68f96060/>Daniela Witten</a></li>
<li><a href=/posts/b2981e3a-4e5b-41b2-a040-2fb58a7735a5/>Trevor Hastie</a></li>
<li><a href=/posts/29b3cfe2-55ed-45d5-92e5-e604808b72bb/>Robert Tibshirani</a></li>
</ol>
<h3 id=aka>AKA<a hidden class=anchor aria-hidden=true href=#aka>¶</a></h3>
<table>
<thead>
<tr>
<th>position</th>
<th>ease</th>
<th>box</th>
<th>interval</th>
<th>due</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>2.65</td>
<td>9</td>
<td>415.17</td>
<td>2024-05-12T22:01:21Z</td>
</tr>
<tr>
<td>1</td>
<td>2.80</td>
<td>11</td>
<td>330.77</td>
<td>2023-12-13T20:30:23Z</td>
</tr>
</tbody>
</table>
<ul>
<li>{{Introduction to Statistical Learning with Applications in R}@0}</li>
<li>{{ISLR}@1}</li>
</ul>
<h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>¶</a></h2>
<style>.csl-entry{text-indent:-1.5em;margin-left:1.5em}</style><div class=csl-bib-body>
<div class=csl-entry><a id=citeproc_bib_item_1></a>James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani, eds. 2013. <i>An Introduction to Statistical Learning: With Applications in R</i>. Springer Texts in Statistics 103. New York: Springer.</div>
</div>
<h2 id=backlinks>Backlinks<a hidden class=anchor aria-hidden=true href=#backlinks>¶</a></h2>
<ul>
<li><a href=/posts/29b3cfe2-55ed-45d5-92e5-e604808b72bb/>Robert Tibshirani</a></li>
<li><a href=/posts/b2981e3a-4e5b-41b2-a040-2fb58a7735a5/>Trevor Hastie</a></li>
<li><a href=/posts/23a21efb-912c-46ff-84f6-5b3d68f96060/>Daniela Witten</a></li>
<li><a href=/posts/f5ed47e7-5d7a-4d4f-9ed2-6817ca706b05/>Gareth James</a></li>
</ul>
</div>
<footer class=post-footer>
<script>var nextUntil=function(a,d,b){var c=[];for(a=a.nextElementSibling;a;){if(a.matches(d))break;if(b&&!a.matches(b)){a=a.nextElementSibling;continue}c.push(a),a=a.nextElementSibling}return c};const flashcardHeader=document.getElementById("flashcards");if(flashcardHeader){const e=nextUntil(flashcardHeader,"h2"),a=document.createElement('div');a.setAttribute('class','toc flashcards');const b=document.createElement('details');a.appendChild(b);const d=document.createElement('summary');b.appendChild(d);const c=document.createElement('div');c.setAttribute('class','inner'),b.appendChild(c),nextUntil(flashcardHeader,"h2").forEach(a=>{c.appendChild(a)}),flashcardHeader.parentNode.insertBefore(document.createElement('hr'),flashcardHeader),flashcardHeader.parentNode.insertBefore(a,flashcardHeader),d.appendChild(flashcardHeader)}</script>
<script>const backlinkHeader=document.getElementById("backlinks");if(backlinkHeader){const b=document.createElement('h4');b.innerHTML=backlinkHeader.innerHTML;const c=backlinkHeader.nextElementSibling;console.log(c);const a=document.createElement('aside');a.setAttribute('class','backlinks toc side right'),backlinkHeader.parentNode.insertBefore(a,backlinkHeader),a.appendChild(b),a.appendChild(c),backlinkHeader.remove(),b.id="backlinks"}</script>
<script src=/js/citations/custom.js></script>
</footer>
</article>
</main>
<footer class=footer>
<span>&copy; 2023 <a href=http://notes.cashpw.com/>Cash Prokop-Weaver</a></span><span style=display:inline-block;margin-left:1em>
<a href=https://creativecommons.org/licenses/by-sa/4.0/>CC BY-SA</a>
</span>
<span style=display:inline-block;margin-left:1em>
Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
    <a href=https://github.com/reorx/hugo-PaperModX/ rel=noopener target=_blank>PaperModX</a>
</span>
</footer>
<a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a>
<script>(function(){const b=''=='1';if(b)return;let a=document.getElementById("theme-toggle");a.removeEventListener('click',toggleThemeListener),a.addEventListener('click',toggleThemeListener)})()</script>
<script>(function(){let a=document.getElementById('menu');a&&(a.scrollLeft=localStorage.getItem("menu-scroll-position"),a.onscroll=function(){localStorage.setItem("menu-scroll-position",a.scrollLeft)});const b=''=='1',c=''=='1';if(window.matchMedia('(prefers-reduced-motion: reduce)').matches||b||c)return;document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})})})()</script>
<script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script>
<script>if(window.scrollListeners)for(const a of scrollListeners)window.removeEventListener('scroll',a);window.scrollListeners=[]</script>
<script src=/js/medium-zoom.min.js data-no-instant></script>
<script>document.querySelectorAll('pre > code').forEach(b=>{const c=b.parentNode.parentNode,a=document.createElement('button');a.classList.add('copy-code'),a.innerText='copy';function d(){a.innerText='copied!',setTimeout(()=>{a.innerText='copy'},2e3)}a.addEventListener('click',e=>{if('clipboard'in navigator){navigator.clipboard.writeText(b.textContent),d();return}const a=document.createRange();a.selectNodeContents(b);const c=window.getSelection();c.removeAllRanges(),c.addRange(a);try{document.execCommand('copy'),d()}catch(a){}c.removeRange(a)}),c.classList.contains("highlight")?c.appendChild(a):c.parentNode.firstChild==c||(b.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?b.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(a):b.parentNode.appendChild(a))})</script>
<script>(function(){const h='1'=='1';if(!h)return;if(!document.querySelector('.toc')){console.log('no toc found, ignore toc scroll');return}const i=window.scrollListeners,c=document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id]'),d='active';let a=c[0];e(a).classList.add(d);const g=()=>{const b=[];for(const a of c)if(j(a)<5)b.push(a);else break;b.length>0?newActiveHeading=b[b.length-1]:newActiveHeading=c[0],a!=newActiveHeading&&(e(a).classList.remove(d),a=newActiveHeading,e(a).classList.add(d))};let b=null;const f=()=>{b!==null&&clearTimeout(b),b=setTimeout(g,50)};window.addEventListener('scroll',f,!1),i.push(f);function e(a){const b=encodeURI(a.getAttribute('id')).toLowerCase();return document.querySelector(`.toc ul li a[href="#${b}"]`)}function j(a){if(!a.getClientRects().length)return 0;let b=a.getBoundingClientRect();return b.top}})()</script>
</body>
</html>